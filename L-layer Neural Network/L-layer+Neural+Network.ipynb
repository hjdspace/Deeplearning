{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L-layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, layers):\n",
    "        \"\"\"\n",
    "        layers: python array (list) containing the dimensions of each layer in our network\n",
    "        for example: layers = [2, 3, 4, 1] is a three-layer NN which input layer has two features\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.L = len(layers) - 1 # the number of neural network layers should exclude input layer\n",
    "        \n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Initial parameters (W, b) and store each of them in a parameters dictionary.\n",
    "        for example: parameters[\"W1\"], parameters[\"b1\"], parameters[\"W2\"], parameters[\"b2\"]\n",
    "        W[l] is the shape of (n[l], n[l-1]), b[l] is the shape of (n[l], 1)\n",
    "        \n",
    "        Arguments:\n",
    "            - epsilon: a constant to weights\n",
    "        Return:\n",
    "            - parameters: a python dict which contains the weights and biases\n",
    "        \"\"\"\n",
    "        #np.random.seed(1)\n",
    "        parameters = {}\n",
    "        for l in range(1, self.L+1):\n",
    "            # implement \"He\" initialization to weights\n",
    "            parameters[\"W\" + str(l)] = np.random.randn(self.layers[l], self.layers[l-1]) * np.sqrt(2 / self.layers[l-1])\n",
    "            parameters[\"b\" + str(l)] = np.zeros((self.layers[l], 1))\n",
    "            \n",
    "            assert(parameters[\"W\" + str(l)].shape == (self.layers[l], self.layers[l-1]))\n",
    "            assert(parameters[\"b\" + str(l)].shape == (self.layers[l], 1))\n",
    "        \n",
    "        assert(len(parameters) // 2 == self.L)\n",
    "            \n",
    "        return parameters\n",
    "    \n",
    "    def parameters_to_vector(self, parameters):\n",
    "        \"\"\"\n",
    "        Roll all parameters dictionary into a single vector\n",
    "        \"\"\"\n",
    "        keys = []\n",
    "        count = 0\n",
    "        for key in sorted(parameters.keys()):\n",
    "            # flatten parameters\n",
    "            new_vector = np.reshape(parameters[key], (-1, 1))\n",
    "            keys += [key] * new_vector.shape[0]\n",
    "            \n",
    "            if count == 0:\n",
    "                theta = new_vector\n",
    "            else:\n",
    "                theta = np.concatenate((theta, new_vector), axis=0)\n",
    "            count += 1\n",
    "        return theta, sorted(keys)\n",
    "    \n",
    "    def vector_to_parameters(self, theta, keys):\n",
    "        \"\"\"\n",
    "        Convert the parameters vector back to dictionary\n",
    "        \"\"\"\n",
    "        parameters = {}\n",
    "        for i, key in enumerate(keys):\n",
    "            parameters.setdefault(key, []).append(list(theta[i]))\n",
    "        for l in range(1, self.L + 1):\n",
    "            parameters[\"W\" + str(l)] = np.array(parameters[\"W\" + str(l)]).reshape(self.layers[l], self.layers[l-1])\n",
    "            parameters[\"b\" + str(l)] = np.array(parameters[\"b\" + str(l)]).reshape(self.layers[l], 1)\n",
    "            \n",
    "        return parameters\n",
    "    \n",
    "    def gradient_to_vector(self, grads):\n",
    "        \"\"\"\n",
    "        Convert the gradient dictionary dW/db to a single vector\n",
    "        \"\"\"\n",
    "        keys = list(grads.keys())\n",
    "        dW_db_keys = []\n",
    "        # remove dZ and dA keys, only keep dW and db keys\n",
    "        import re\n",
    "        for key in keys:\n",
    "            if re.search(r\"d(W|b)\", key, re.I) is not None:\n",
    "                dW_db_keys.append(key)\n",
    "        count = 0\n",
    "        for key in sorted(dW_db_keys):\n",
    "            # flatten dW db gradient\n",
    "            new_vector = np.reshape(grads[key], (-1, 1))\n",
    "            if count == 0:\n",
    "                theta = new_vector\n",
    "            else:\n",
    "                theta = np.concatenate((theta, new_vector), axis=0)\n",
    "            count += 1\n",
    "        return theta, sorted(dW_db_keys)\n",
    "    \n",
    "    def sigmoid(self, Z):\n",
    "        \"\"\"\n",
    "        Sigmoid activation function for the output layer\n",
    "        \"\"\"\n",
    "        Activations = 1. / (1 + np.exp(-Z))\n",
    "        return Activations\n",
    "    \n",
    "    def relu(self, Z):\n",
    "        \"\"\"\n",
    "        RELU activation function for the hidden layer\n",
    "        \"\"\"\n",
    "        Activations = np.maximum(0, Z)\n",
    "        return Activations\n",
    "    \n",
    "    def forward_propagation(self, X, parameters):\n",
    "        \"\"\"\n",
    "        Implement the forward propagation for the entire neural network\n",
    "        Arguments:\n",
    "            - X: numpy array of shape (number of features, number of examples)\n",
    "            - parameters: a python dict which contain the weights and biases\n",
    "        Return:\n",
    "            - Z: a list of linear forward function for each layer,start from Z[1] (Z[0] is None)\n",
    "            - A: a list of activations of Z, A[0] = X\n",
    "        \"\"\"\n",
    "        A = []\n",
    "        Z = []\n",
    "        A.append(X) # make A[0] = X\n",
    "        Z.append(None) # make Z[0] = None (not used)\n",
    "        for l in range(1, self.L + 1):\n",
    "            Z.append(np.dot(parameters[\"W\" + str(l)], A[l-1]) + parameters[\"b\"+ str(l)])\n",
    "            if l == self.L:\n",
    "                A.append(self.sigmoid(Z[l]))\n",
    "            else:\n",
    "                A.append(self.relu(Z[l]))\n",
    "        \n",
    "        assert(len(Z) == self.L + 1), print(\"The dimension of Z is wrong\")\n",
    "        assert(len(A) == self.L + 1), print(\"The dimension of A is wrong\")\n",
    "        return A, Z\n",
    "    \n",
    "    def forward_propagation_with_dropout(self, X, parameters, keep_prob=0.5):\n",
    "        \"\"\"\n",
    "        Implements the forward propagation with dropout\n",
    "    \n",
    "        Arguments:\n",
    "        X -- input dataset, of shape (number of features, number of examples)\n",
    "        parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\",..., \"WL\", \"bL\":\n",
    "        keep_prob - probability of keeping a neuron active during drop-out, scalar\n",
    "    \n",
    "        Returns:\n",
    "        A -- a list of linear forward function for each layer,start from Z[1] (Z[0] is None)\n",
    "        Z -- a list of activations of Z, A[0] = X\n",
    "        D -- a list of dropout matrix, D[0] = None\n",
    "        \"\"\"\n",
    "        np.random.seed(1)\n",
    "        A = []\n",
    "        Z = []\n",
    "        D = []\n",
    "        A.append(X) # make A[0] = X\n",
    "        Z.append(None) # make Z[0] = None (not used)\n",
    "        D.append(None)\n",
    "        for l in range(1, self.L + 1):\n",
    "            Z.append(np.dot(parameters[\"W\" + str(l)], A[l-1]) + parameters[\"b\"+ str(l)])\n",
    "            if l == self.L:\n",
    "                A.append(self.sigmoid(Z[l]))\n",
    "            else:\n",
    "                A.append(self.relu(Z[l]))\n",
    "            # Apply dropout to hidden unit\n",
    "            if l is not self.L:\n",
    "                Dl = np.random.rand(A[l].shape[0], A[l].shape[1])  # Step 1: initialize matrix Dl = np.random.rand(..., ...)\n",
    "                Dl = (Dl < keep_prob)                              # Step 2: convert entries of Dl to 0 or 1 (using keep_prob as the threshold)\n",
    "                A[l] = np.multiply(A[l], Dl)                       # Step 3: shut down some neurons of Al\n",
    "                A[l] = A[l] / keep_prob                            # Step 4: scale the value of neurons that haven't been shut down\n",
    "                D.append(Dl)\n",
    "        return A, Z, D             \n",
    "    \n",
    "    def compute_cost(self, Y, AL):\n",
    "        \"\"\"\n",
    "        Implement the cost function\n",
    "        Argument:\n",
    "            - Y: the true \"label\" output eache sample, shape (1, number of examples)\n",
    "            - AL: the predict output, shape (1, number of examples)\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "        cost = (-1 / m) * np.sum(Y * np.log(AL) + (1 - Y) * np.log(1-AL))\n",
    "        cost= np.squeeze(cost) # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17)\n",
    "        assert(cost.shape == ())\n",
    "        return cost\n",
    "    \n",
    "    def compute_cost_with_regularization(self, Y, AL, parameters, lambd):\n",
    "        \"\"\"\n",
    "        Implement the cost function with L2 regularization\n",
    "        Argument:\n",
    "            - Y: the true \"label\" output eache sample, shape (1, number of examples)\n",
    "            - AL: the predict output, shape (1, number of examples)\n",
    "            - parameters: python dictionary containing parameters of the model\n",
    "            - lambd: regularization hyperparameter, scalar\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "        l2_regularization_cost = 0\n",
    "        for l in range(1, self.L+1):\n",
    "            l2_regularization_cost += (lambd / (2 * m)) * np.sum(np.square(parameters[\"W\" + str(l)]))\n",
    "        cost = self.compute_cost(Y, AL) + l2_regularization_cost\n",
    "        return cost\n",
    "    \n",
    "    def sigmoid_backward(self, dA, Z):\n",
    "        \"\"\"\n",
    "        Implements the backward propagation for SIGMOID unit. \n",
    "        \"\"\"\n",
    "        s = 1. / (1 + np.exp(-Z))\n",
    "        dZ = dA * s * (1 - s)\n",
    "        assert(dZ.shape == Z.shape)\n",
    "        return dZ\n",
    "    \n",
    "    def relu_backward(self, dA, Z):\n",
    "        \"\"\"\n",
    "        Implements the backward propagation for RELU unit\n",
    "        \"\"\"\n",
    "        dZ = np.array(dA, copy=True)\n",
    "        # When z <= 0, you should set dz to 0 as well. \n",
    "        dZ[Z <= 0] = 0\n",
    "        assert(dZ.shape == Z.shape)\n",
    "        return dZ\n",
    "    \n",
    "    def backward_propagation(self, Y, A, Z, parameters):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation for each layer\n",
    "        Arguments:\n",
    "            - Y: The true outputs of each example\n",
    "            - A: The list of activation function of each layer\n",
    "            - Z: The list linear forward function of each layer\n",
    "            - parameters: the weights and biases of each layer\n",
    "        Return:\n",
    "            - grads: A dictionary with the gradients\n",
    "                grads[\"dA\" + str(l)] = ...\n",
    "                grads[\"dW\" + str(l)] = ...\n",
    "                grads[\"db\" + str(l)] = ...\n",
    "        \"\"\"\n",
    "        grads = {}\n",
    "        m = Y.shape[1]\n",
    "        # Initialize the back propagation\n",
    "        AL = A[self.L]\n",
    "        # Way 1\n",
    "        grads[\"dA\" + str(self.L)] = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "        grads[\"dZ\" + str(self.L)] = self.sigmoid_backward(grads[\"dA\" + str(self.L)], Z[self.L])\n",
    "        grads[\"dW\" + str(self.L)] = (1. / m) * np.dot(grads[\"dZ\" + str(self.L)], A[self.L-1].T)\n",
    "        grads[\"db\" + str(self.L)] = (1. / m) * np.sum(grads[\"dZ\" + str(self.L)], axis=1, keepdims=True)\n",
    "        for l in reversed(range(self.L - 1)):\n",
    "            grads[\"dA\" + str(l + 1)] = np.dot(parameters[\"W\" + str(l + 2)].T, grads[\"dZ\" + str(l + 2)])\n",
    "            grads[\"dZ\" + str(l + 1)] = self.relu_backward(grads[\"dA\" + str(l + 1)], Z[l + 1])\n",
    "            grads[\"dW\" + str(l + 1)] = (1. / m) * np.dot(grads[\"dZ\" + str(l + 1)], A[l].T)\n",
    "            grads[\"db\" + str(l + 1)] = (1. / m) * np.sum(grads[\"dZ\" + str(l + 1)], axis=1, keepdims=True)\n",
    "              \n",
    "        return grads\n",
    "    \n",
    "    def backward_propagation_with_regularization(self, Y, A, Z, parameters, lambd=0.5):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation for each layer\n",
    "        Arguments:\n",
    "            - Y: The true outputs of each example\n",
    "            - A: The list of activation function of each layer\n",
    "            - Z: The list linear forward function of each layer\n",
    "            - parameters: the weights and biases of each layer\n",
    "            - lambd: regularization hyperparameter, scalar\n",
    "        Return:\n",
    "            - grads: A dictionary with the gradients\n",
    "                grads[\"dA\" + str(l)] = ...\n",
    "                grads[\"dW\" + str(l)] = ...\n",
    "                grads[\"db\" + str(l)] = ...\n",
    "        \"\"\"\n",
    "        grads = {}\n",
    "        m = Y.shape[1]\n",
    "        # Initialize the back propagation\n",
    "        AL = A[self.L]\n",
    "        # Way 1\n",
    "        grads[\"dA\" + str(self.L)] = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "        grads[\"dZ\" + str(self.L)] = self.sigmoid_backward(grads[\"dA\" + str(self.L)], Z[self.L])\n",
    "        grads[\"dW\" + str(self.L)] = (1. / m) * np.dot(grads[\"dZ\" + str(self.L)], A[self.L-1].T) + (lambd / m) * parameters[\"W\" + str(self.L)]\n",
    "        grads[\"db\" + str(self.L)] = (1. / m) * np.sum(grads[\"dZ\" + str(self.L)], axis=1, keepdims=True)\n",
    "        for l in reversed(range(self.L - 1)):\n",
    "            grads[\"dA\" + str(l + 1)] = np.dot(parameters[\"W\" + str(l + 2)].T, grads[\"dZ\" + str(l + 2)])\n",
    "            grads[\"dZ\" + str(l + 1)] = self.relu_backward(grads[\"dA\" + str(l + 1)], Z[l + 1])\n",
    "            grads[\"dW\" + str(l + 1)] = (1. / m) * np.dot(grads[\"dZ\" + str(l + 1)], A[l].T) + (lambd / m) * parameters[\"W\" + str(l + 1)]\n",
    "            grads[\"db\" + str(l + 1)] = (1. / m) * np.sum(grads[\"dZ\" + str(l + 1)], axis=1, keepdims=True)\n",
    "              \n",
    "        return grads\n",
    "    \n",
    "    def backward_propagation_with_dropout(self, Y, A, Z, D, parameters, keep_prob=1):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation with dropout for each layer\n",
    "        Arguments:\n",
    "            - Y: The true outputs of each example\n",
    "            - A: The list of activation function of each layer\n",
    "            - Z: The list linear forward function of each layer\n",
    "            - parameters: the weights and biases of each layer\n",
    "            - lambd: regularization hyperparameter, scalar\n",
    "        Return:\n",
    "            - grads: A dictionary with the gradients\n",
    "                grads[\"dA\" + str(l)] = ...\n",
    "                grads[\"dW\" + str(l)] = ...\n",
    "                grads[\"db\" + str(l)] = ...\n",
    "        \"\"\"\n",
    "        grads = {}\n",
    "        m = Y.shape[1]\n",
    "        # Initialize the back propagation\n",
    "        AL = A[self.L]\n",
    "        # Way 1\n",
    "        grads[\"dA\" + str(self.L)] = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "        grads[\"dZ\" + str(self.L)] = self.sigmoid_backward(grads[\"dA\" + str(self.L)], Z[self.L])\n",
    "        grads[\"dW\" + str(self.L)] = (1. / m) * np.dot(grads[\"dZ\" + str(self.L)], A[self.L-1].T)\n",
    "        grads[\"db\" + str(self.L)] = (1. / m) * np.sum(grads[\"dZ\" + str(self.L)], axis=1, keepdims=True)\n",
    "        for l in reversed(range(self.L - 1)):\n",
    "            grads[\"dA\" + str(l + 1)] = np.dot(parameters[\"W\" + str(l + 2)].T, grads[\"dZ\" + str(l + 2)])\n",
    "            grads[\"dA\" + str(l + 1)] = np.multiply(grads[\"dA\" + str(l + 1)], D[l + 1]) / keep_prob\n",
    "            grads[\"dZ\" + str(l + 1)] = self.relu_backward(grads[\"dA\" + str(l + 1)], Z[l + 1])\n",
    "            grads[\"dW\" + str(l + 1)] = (1. / m) * np.dot(grads[\"dZ\" + str(l + 1)], A[l].T)\n",
    "            grads[\"db\" + str(l + 1)] = (1. / m) * np.sum(grads[\"dZ\" + str(l + 1)], axis=1, keepdims=True)\n",
    "              \n",
    "        return grads\n",
    "    \n",
    "    def update_parameters(self, parameters, grads, learning_rate):\n",
    "        \"\"\"\n",
    "        Update parameters using gradient descent\n",
    "    \n",
    "        Arguments:\n",
    "        parameters -- python dictionary containing your parameters \n",
    "        grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "        Returns:\n",
    "        parameters -- python dictionary containing your updated parameters \n",
    "                      parameters[\"W\" + str(l)] = ... \n",
    "                      parameters[\"b\" + str(l)] = ...\n",
    "        \"\"\"\n",
    "        for l in range(self.L):\n",
    "            parameters[\"W\" + str(l+1)] -= learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "            parameters[\"b\" + str(l+1)] -= learning_rate * grads[\"db\" + str(l + 1)]\n",
    "            \n",
    "        return parameters\n",
    "    \n",
    "    def training_(self, X, Y, n_iters=1000, learning_rate=0.01, print_cost=False):\n",
    "        \"\"\"\n",
    "        Training this Neural Network\n",
    "        \"\"\"\n",
    "        #np.random.seed(1)\n",
    "        costs = []\n",
    "        # Initialize parameters\n",
    "        parameters = self.initialize_parameters()\n",
    "        \n",
    "        for i in range(n_iters):\n",
    "            # forward propagation\n",
    "            A, Z = self.forward_propagation(X, parameters)\n",
    "        \n",
    "            # compute cost\n",
    "            cost = self.compute_cost(Y, A[self.L])\n",
    "            \n",
    "            # backward propagation\n",
    "            grads = self.backward_propagation(Y, A, Z, parameters)\n",
    "            \n",
    "            # update parameters\n",
    "            parameters = self.update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "            if print_cost and i % 100 == 0:\n",
    "                print(f\"Cost at iteration {i}: {cost}\")\n",
    "            \n",
    "            if print_cost and i % 100 == 0:\n",
    "                costs.append(cost)\n",
    "        \n",
    "        #pprint.pprint(grads)\n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel(\"cost\")\n",
    "        plt.xlabel(\"Number of iteration\")\n",
    "        plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "        plt.show()\n",
    "                \n",
    "        return parameters\n",
    "    \n",
    "    def training(self, X, Y, n_iters=1000, learning_rate=0.01, print_cost=False, lambd=0, keep_prob=1, gradient_check=False):\n",
    "        \"\"\"\n",
    "        Training this Neural Network\n",
    "        Argument:\n",
    "            X - input data, of shape (input size, number of examples)\n",
    "            Y - true \"label\" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)\n",
    "            learning_rate - learning rate of the optimization\n",
    "            num_iterations - number of iterations of the optimization loop\n",
    "            print_cost - If True, print the cost every 100 iterations\n",
    "            lambd - regularization hyperparameter, scalar\n",
    "            keep_prob - probability of keeping a neuron active during drop-out, scalar\n",
    "            \n",
    "        Returns:\n",
    "            parameters - parameters learned by the model. They can then be used to predict.\n",
    "        \"\"\"\n",
    "        #np.random.seed(1)\n",
    "        costs = []\n",
    "        m = X.shape[1]\n",
    "        # Initialize parameters\n",
    "        parameters = self.initialize_parameters()\n",
    "        \n",
    "        for i in range(n_iters):\n",
    "            # forward propagation\n",
    "            if keep_prob == 1:\n",
    "                A, Z = self.forward_propagation(X, parameters)\n",
    "            elif keep_prob < 1:\n",
    "                A, Z, D = self.forward_propagation_with_dropout(X, parameters, keep_prob)\n",
    "        \n",
    "            # compute cost\n",
    "            if lambd == 0:\n",
    "                cost = self.compute_cost(Y, A[-1])\n",
    "            else:\n",
    "                cost = self.compute_cost_with_regularization(Y, A[-1], parameters, lambd)\n",
    "            \n",
    "            # backward propagation\n",
    "            if lambd == 0 and keep_prob == 1:\n",
    "                grads = self.backward_propagation(Y, A, Z, parameters)\n",
    "            elif lambd != 0:\n",
    "                grads = self.backward_propagation_with_regularization(Y, A, Z, parameters, lambd)\n",
    "            elif keep_prob < 1:\n",
    "                grads = self.backward_propagation_with_dropout(Y, A, Z, D, parameters, keep_prob)\n",
    "            \n",
    "            # Apply gradient check for several iterations\n",
    "            if gradient_check and i in range(3):\n",
    "                self.gradient_check(parameters, grads, X, Y)\n",
    "                \n",
    "            # update parameters\n",
    "            parameters = self.update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "            if print_cost and i % 10000 == 0:\n",
    "                print(f\"Cost at iteration {i}: {cost}\")\n",
    "            \n",
    "            if print_cost and i % 1000 == 0:\n",
    "                costs.append(cost)\n",
    "        \n",
    "        #pprint.pprint(grads)\n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel(\"cost\")\n",
    "        plt.xlabel(\"Number of iteration\")\n",
    "        plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "        plt.show()\n",
    "                \n",
    "        return parameters\n",
    "    \n",
    "    def gradient_check(self, parameters, gradients, X, Y, epsilon=1e-7):\n",
    "        \"\"\"\n",
    "        Checks if backward_propagation computes correctly the gradient of the cost output by forward_propagation\n",
    "    \n",
    "        Arguments:\n",
    "            parameters -- python dictionary containing your parameters\n",
    "            gradients -- output of backward_propagation, contains gradients of the cost with respect to the parameters. \n",
    "            X -- input data, of shape (input size, number of examples)\n",
    "            Y - true \"label\" vector, of shape (output size, number of examples)\n",
    "            epsilon -- tiny shift to the input to compute approximated gradient\n",
    "    \n",
    "        Returns:\n",
    "            difference -- difference between the approximated gradient and the backward propagation gradient\n",
    "        \"\"\"\n",
    "        # dict to vector\n",
    "        parameters_values, keys = self.parameters_to_vector(parameters)\n",
    "        grads_values, _ = self.gradient_to_vector(gradients)\n",
    "        num_parameters = parameters_values.shape[0]\n",
    "        J_plus = np.zeros((num_parameters, 1))\n",
    "        J_minus = np.zeros((num_parameters, 1))\n",
    "        gradapprox = np.zeros((num_parameters, 1))\n",
    "        \n",
    "        # Compute gradapprox\n",
    "        for i in range(num_parameters):\n",
    "            # Compute J_plus[i]\n",
    "            thetaplus = np.copy(parameters_values)\n",
    "            thetaplus[i][0] += epsilon\n",
    "            Aplus, _ = self.forward_propagation(X, self.vector_to_parameters(thetaplus, keys))\n",
    "            J_plus[i] = self.compute_cost(Y, Aplus[-1])\n",
    "            \n",
    "            # Compute J_minus[i]\n",
    "            thetaminus = np.copy(parameters_values)\n",
    "            thetaminus[i][0] -= epsilon\n",
    "            Aminus, _ = self.forward_propagation(X, self.vector_to_parameters(thetaminus, keys))\n",
    "            J_minus[i] = self.compute_cost(Y, Aminus[-1])\n",
    "            \n",
    "            gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)\n",
    "            \n",
    "        numerator = np.linalg.norm(gradapprox - grads_values)\n",
    "        denominator = np.linalg.norm(gradapprox) + np.linalg.norm(grads_values)\n",
    "        difference = numerator / denominator\n",
    "        \n",
    "        if difference > 1e-7:\n",
    "            print(\"There is a mistake in backward propagation! difference = \" + str(difference))\n",
    "        else:\n",
    "            print (\"Your backward propagation works perfectly fine! difference = \" + str(difference))\n",
    "            \n",
    "        return difference\n",
    "    \n",
    "    def predict(self, X, Y, parameters):\n",
    "        \"\"\"\n",
    "        Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "        Arguments:\n",
    "        parameters -- python dictionary containing your parameters \n",
    "        X -- input data of size (n_x, m)\n",
    "        Y -- the true label of examples\n",
    "    \n",
    "        Returns\n",
    "        predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "        A, Z = self.forward_propagation(X, parameters)\n",
    "        Y_predict = A[self.L]\n",
    "        predictions = np.zeros((1, m))\n",
    "        #predictions = np.array([1 if elem > 0.5 else 0 for elem in Y_predict[0, :]])\n",
    "        for i in range(0, Y_predict.shape[1]):\n",
    "            if Y_predict[0, i] > 0.5:\n",
    "                predictions[0, i] = 1\n",
    "            else:\n",
    "                predictions[0, i] = 0\n",
    "        print(\"Accuracy: \"  + str(np.sum((predictions == Y)/m)))\n",
    "        return predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
